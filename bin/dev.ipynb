{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Iterator, List, Mapping, Optional\n",
    "\n",
    "from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from langchain_core.outputs import GenerationChunk\n",
    "\n",
    "class CustomLLM(LLM):\n",
    "    \"\"\"A custom chat model that echoes the first `n` characters of the input.\n",
    "\n",
    "    When contributing an implementation to LangChain, carefully document\n",
    "    the model including the initialization parameters, include\n",
    "    an example of how to initialize the model and include any relevant\n",
    "    links to the underlying models documentation or API.\n",
    "\n",
    "    Example:\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            model = CustomChatModel(n=2)\n",
    "            result = model.invoke([HumanMessage(content=\"hello\")])\n",
    "            result = model.batch([[HumanMessage(content=\"hello\")],\n",
    "                                 [HumanMessage(content=\"world\")]])\n",
    "    \"\"\"\n",
    "\n",
    "    n: int\n",
    "    \"\"\"The number of characters from the last message of the prompt to be echoed.\"\"\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        \"\"\"Run the LLM on the given input.\n",
    "\n",
    "        Override this method to implement the LLM logic.\n",
    "\n",
    "        Args:\n",
    "            prompt: The prompt to generate from.\n",
    "            stop: Stop words to use when generating. Model output is cut off at the\n",
    "                first occurrence of any of the stop substrings.\n",
    "                If stop tokens are not supported consider raising NotImplementedError.\n",
    "            run_manager: Callback manager for the run.\n",
    "            **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
    "                to the model provider API call.\n",
    "\n",
    "        Returns:\n",
    "            The model output as a string. Actual completions SHOULD NOT include the prompt.\n",
    "        \"\"\"\n",
    "        if stop is not None:\n",
    "            raise ValueError(\"stop kwargs are not permitted.\")\n",
    "        return prompt[: self.n]\n",
    "\n",
    "    def _stream(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Iterator[GenerationChunk]:\n",
    "        \"\"\"Stream the LLM on the given prompt.\n",
    "\n",
    "        This method should be overridden by subclasses that support streaming.\n",
    "\n",
    "        If not implemented, the default behavior of calls to stream will be to\n",
    "        fallback to the non-streaming version of the model and return\n",
    "        the output as a single chunk.\n",
    "\n",
    "        Args:\n",
    "            prompt: The prompt to generate from.\n",
    "            stop: Stop words to use when generating. Model output is cut off at the\n",
    "                first occurrence of any of these substrings.\n",
    "            run_manager: Callback manager for the run.\n",
    "            **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
    "                to the model provider API call.\n",
    "\n",
    "        Returns:\n",
    "            An iterator of GenerationChunks.\n",
    "        \"\"\"\n",
    "        for char in prompt[: self.n]:\n",
    "            chunk = GenerationChunk(text=char)\n",
    "            if run_manager:\n",
    "                run_manager.on_llm_new_token(chunk.text, chunk=chunk)\n",
    "\n",
    "            yield chunk\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return a dictionary of identifying parameters.\"\"\"\n",
    "        return {\n",
    "            # The model name allows users to specify custom token counting\n",
    "            # rules in LLM monitoring applications (e.g., in LangSmith users\n",
    "            # can provide per token pricing for their model and monitor\n",
    "            # costs for the given LLM.)\n",
    "            \"model_name\": \"CustomChatModel\",\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Get the type of language model used by this chat model. Used for logging purposes only.\"\"\"\n",
    "        return \"custom\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "\n",
    "from vertexai.generative_models import GenerativeModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# TODO(developer): Update and un-comment below line\n",
    "# PROJECT_ID = \"your-project-id\"\n",
    "vertexai.init(project=PROJECT_ID, location=\"us-central1\")\n",
    "\n",
    "model = GenerativeModel(\"gemini-1.5-flash-002\")\n",
    "responses = model.generate_content(\n",
    "    \"Write a story about a magic backpack.\", stream=True\n",
    ")\n",
    "\n",
    "for response in responses:\n",
    "    print(response.text)\n",
    "# Example response:\n",
    "# El\n",
    "# ara wasn't looking for magic. She was looking for rent money.\n",
    "# Her tiny apartment, perched precariously on the edge of Whispering Woods,\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gemini:\n",
    "    def __init__(self, url=None, isLog=False):\n",
    "        \n",
    "        self.url = url\n",
    "        self.max_output = 8192\n",
    "        self.temperature = 0\n",
    "        self.is_log = isLog\n",
    "        \n",
    "        with open(\"/Users/sirabhobs/Desktop/poc-hr-helpdesk-chatbot/credential/ktb-complaint-center-poc-d47fde693217.json\", \"r\") as fr:\n",
    "            credentials = fr.read()\n",
    "            \n",
    "        self.token = self.getToken(credentials)\n",
    "\n",
    "    @staticmethod\n",
    "    def getToken(credential):\n",
    "        \n",
    "        url = 'http://10.9.93.83:8443/google-authen'\n",
    "        data = {'service_account': credential}\n",
    "        headers = {'Content-Type': 'application/json'}\n",
    "        response = requests.post(url, data, headers)\n",
    "        response_json = response.json()\n",
    "\n",
    "        return response_json['Token']\n",
    "\n",
    "    def call_gemini(self, prompt, isJsonOutput=False):\n",
    "        \"\"\"Synchronous call to Gemini API\"\"\"\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.token}\",\n",
    "            \"Content-Type\": \"application/json\",\n",
    "        }\n",
    "\n",
    "        data = {\n",
    "            \"contents\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"parts\": [{\"text\": prompt}],\n",
    "                }\n",
    "            ],\n",
    "            \"generation_config\": {\n",
    "                \"temperature\": 0,\n",
    "                \"topP\": 1, \n",
    "                \"seed\": 42\n",
    "            },\n",
    "            \"safetySettings\": [\n",
    "                {\n",
    "                \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "                \"threshold\": \"OFF\"\n",
    "                },\n",
    "                {\n",
    "                \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "                \"threshold\": \"OFF\"\n",
    "                },\n",
    "                {\n",
    "                \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "                \"threshold\": \"OFF\"\n",
    "                },\n",
    "                {\n",
    "                \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "                \"threshold\": \"OFF\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        if isJsonOutput:\n",
    "            data[\"generation_config\"][\"responseMimeType\"] = \"application/json\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(self.url, headers=headers, json=data, timeout=300)\n",
    "            return self.process_response(response, prompt)\n",
    "\n",
    "        except (requests.RequestException, IndexError, KeyError, TypeError) as e:\n",
    "            if self.is_log:\n",
    "                CustomLogging.log_error(f\"Request failed or unexpected response structure: {e}\")\n",
    "\n",
    "    def process_response(self, response, prompt):\n",
    "        \"\"\"Processes the API response and logs information\"\"\"\n",
    "        \n",
    "        result = response.json()\n",
    "        candidates = [\n",
    "            candidate.get(\"content\", {}).get(\"parts\", [])[0].get(\"text\", \"\")\n",
    "            for line in result\n",
    "            if \"candidates\" in line\n",
    "            for candidate in line[\"candidates\"]\n",
    "        ]\n",
    "        \n",
    "        content = \"\".join(filter(None, candidates))\n",
    "\n",
    "        usage_metadata = result[-1].get(\"usageMetadata\", {})\n",
    "        \n",
    "        token_usage = {\n",
    "            \"promptTokenCount\": usage_metadata.get(\"promptTokenCount\", 0),\n",
    "            \"candidatesTokenCount\": usage_metadata.get(\"candidatesTokenCount\", 0),\n",
    "            \"totalTokenCount\": usage_metadata.get(\"totalTokenCount\", 0),\n",
    "        }\n",
    "        \n",
    "        llm_info = {\n",
    "            'prompt': prompt,\n",
    "            'llmModel': result[0].get('modelVersion', ''),\n",
    "            'temperature': self.temperature,\n",
    "            'llmResponse': content,\n",
    "            'processingTime': response.elapsed.total_seconds() * 1000,  # Convert to milliseconds\n",
    "            **token_usage\n",
    "        }\n",
    "\n",
    "        if self.is_log:\n",
    "            CustomLogging.log_llm_info(llm_info)\n",
    "\n",
    "        return content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
